{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started Welcome to NHT Labs. Info Estimated time to complete the labs is as follows: DIY Foundation - 60 minutes Prism Central - 30 minutes XRAY - 60 minutes What's New Workshop uses for the following software versions: AOS 5.20.3 Prism Central pc.2022.4.x Agenda DIY Foundation Deploying Prism Central XRAY Initial Setup Take note of the Passwords being used from you RX reservation details Log into your virtual desktops (connection info below) Login to Global Protect VPN if you have access Cluster Assignment The instructor will inform the attendees their assigned clusters. Note If these are Single Node Clusters (SNCs) pay close attention on the networking part. The SNCs are setup and configured differently to the 3 or 4 node clusters Environment Details Nutanix Workshops are intended to be run in the Nutanix Hosted POC environment. Your cluster will be provisioned with all necessary images,networks, and VMs required to complete the exercises. Networking As we are able to provide three/four node clusters and single node clusters in the HPOC environment, we need to describe each sort of cluster separately. The clusters are setup and configured differently. Three/Four node HPOC clusters Three or four node Hosted POC clusters follow a standard naming convention: Cluster Name - POC XYZ Subnet - 10. 42 . XYZ .0 Cluster IP - 10. 42 . XYZ .37 For example: Cluster Name - POC055 Subnet - 10.42.55.0 Cluster IP - 10.42.55.37 for the VIP of the Cluster Throughout the Workshop there are multiple instances where you will need to substitute XYZ with the correct octet for your subnet, for example: IP Address Description 10.42. XYZ .37 Nutanix Cluster Virtual IP 10.42. XYZ .39 PC VM IP, Prism Central 10.42. XYZ .41 DC VM IP, NTNXLAB.local Domain Controller Each cluster is configured with 2 VLANs which can be used for VMs: Network Name Address VLAN DHCP Scope Primary 10.42. XYZ .1/25 0 10.42. XYZ .50-10.42. XYZ .124 Secondary 10.42. XYZ .129/25 XYZ1 10.42. XYZ .132-10.42. XYZ .253 Single Node HPOC Clusters For some workshops we are using Single Node Clusters (SNC). The reason for this is to allow more people to have a dedicated cluster but still have enough free clusters for the bigger workshops including those for customers. The network in the SNC config is using a /26 network. This splits the network address into four equal sizes that can be used for workshops. The below table describes the setup of the network in the four partitions. It provides essential information for the workshop with respect to the IP addresses and the services running at that IP address. Partition 1 Partition 2 Partition 3 Partition 4 Service Comment 10.38.x.1 10.38.x.65 10.38.x.129 10.38.x.193 Gateway 10.38.x.5 10.38.x.69 10.38.x.133 10.38.x.197 AHV Host 10.38.x.6 10.38.x.70 10.38.x.134 10.38.x.198 CVM IP 10.38.x.7 10.38.x.71 10.38.x.135 10.38.x.199 Cluster IP 10.38.x.8 10.38.x.72 10.38.x.136 10.38.x.200 Data Services IP 10.38.x.9 10.38.x.73 10.38.x.137 10.38.x.201 Prism Central IP 10.38.x.11 10.38.x.75 10.38.x.139 10.38.x.203 AutoDC IP(DC) 10.38.x.32-37 10.38.x.96-101 10.38.x.160-165 10.38.x.224-229 Objects 1 10.38.x.38-58 10.38.x.102-122 10.38.x.166-186 10.38.x.230-250 Primary 6 Free IPs for static assignment Credentials Note The Cluster Password is unique to each cluster and will be provided by the leader of the Workshop. Credential Username Password Prism Element admin Cluster Password Prism Central admin Cluster Password Controller VM nutanix Cluster Password Prism Central VM nutanix Cluster Password Each cluster has a dedicated domain controller VM, DC , responsible for providing AD services for the NTNXLAB.local domain. The domain is populated with the following Users and Groups: Group Username(s) Password Administrators Administrator nutanix/4u SSP Admins adminuser01-adminuser25 nutanix/4u SSP Developers devuser01-devuser25 nutanix/4u SSP Consumers consumer01-consumer25 nutanix/4u SSP Operators operator01-operator25 nutanix/4u SSP Custom custom01-custom25 nutanix/4u Bootcamp Users user01-user25 nutanix/4u Access Instructions The Nutanix Hosted POC environment can be accessed a number of different ways: Lab Access User Credentials PHX Based Clusters: Username: PHX-POCxxx-User01 (up to PHX-POCxxx-User20), Password: Provided by Instructor RTP Based Clusters: Username: RTP-POCxxx-User01 (up to RTP-POCxxx-User20), Password: Provided by Instructor Frame VDI Login to: https://console.nutanix.com/x/labs Nutanix Employees - Use your NUTANIXDC credentials Non-Employees - Use Lab Access User Credentials Parallels VDI PHX Based Clusters Login to: https://xld-uswest1.nutanix.com RTP Based Clusters Login to: https://xld-useast1.nutanix.com Nutanix Employees - Use your NUTANIXDC credentials Non-Employees - Use Lab Access User Credentials Employee Pulse Secure VPN Download the client: PHX Based Clusters Login to: https://xld-uswest1.nutanix.com RTP Based Clusters Login to: https://xld-useast1.nutanix.com Nutanix Employees - Use your NUTANIXDC credentials Non-Employees - Use Lab Access User Credentials Install the client. In Pulse Secure Client, Add a connection: For PHX: Type - Policy Secure (UAC) or Connection Server Name - X-Labs - PHX Server URL - xlv-uswest1.nutanix.com For RTP: Type - Policy Secure (UAC) or Connection Server Name - X-Labs - RTP Server URL - xlv-useast1.nutanix.com Nutanix Version Info AOS 5.20.3 Prism Central pc.2022.4.x","title":"Home"},{"location":"#getting-started","text":"Welcome to NHT Labs. Info Estimated time to complete the labs is as follows: DIY Foundation - 60 minutes Prism Central - 30 minutes XRAY - 60 minutes","title":"Getting Started"},{"location":"#whats-new","text":"Workshop uses for the following software versions: AOS 5.20.3 Prism Central pc.2022.4.x","title":"What's New"},{"location":"#agenda","text":"DIY Foundation Deploying Prism Central XRAY","title":"Agenda"},{"location":"#initial-setup","text":"Take note of the Passwords being used from you RX reservation details Log into your virtual desktops (connection info below) Login to Global Protect VPN if you have access","title":"Initial Setup"},{"location":"#cluster-assignment","text":"The instructor will inform the attendees their assigned clusters. Note If these are Single Node Clusters (SNCs) pay close attention on the networking part. The SNCs are setup and configured differently to the 3 or 4 node clusters","title":"Cluster Assignment"},{"location":"#environment-details","text":"Nutanix Workshops are intended to be run in the Nutanix Hosted POC environment. Your cluster will be provisioned with all necessary images,networks, and VMs required to complete the exercises.","title":"Environment Details"},{"location":"#networking","text":"As we are able to provide three/four node clusters and single node clusters in the HPOC environment, we need to describe each sort of cluster separately. The clusters are setup and configured differently.","title":"Networking"},{"location":"#threefour-node-hpoc-clusters","text":"Three or four node Hosted POC clusters follow a standard naming convention: Cluster Name - POC XYZ Subnet - 10. 42 . XYZ .0 Cluster IP - 10. 42 . XYZ .37 For example: Cluster Name - POC055 Subnet - 10.42.55.0 Cluster IP - 10.42.55.37 for the VIP of the Cluster Throughout the Workshop there are multiple instances where you will need to substitute XYZ with the correct octet for your subnet, for example: IP Address Description 10.42. XYZ .37 Nutanix Cluster Virtual IP 10.42. XYZ .39 PC VM IP, Prism Central 10.42. XYZ .41 DC VM IP, NTNXLAB.local Domain Controller Each cluster is configured with 2 VLANs which can be used for VMs: Network Name Address VLAN DHCP Scope Primary 10.42. XYZ .1/25 0 10.42. XYZ .50-10.42. XYZ .124 Secondary 10.42. XYZ .129/25 XYZ1 10.42. XYZ .132-10.42. XYZ .253","title":"Three/Four node HPOC clusters"},{"location":"#single-node-hpoc-clusters","text":"For some workshops we are using Single Node Clusters (SNC). The reason for this is to allow more people to have a dedicated cluster but still have enough free clusters for the bigger workshops including those for customers. The network in the SNC config is using a /26 network. This splits the network address into four equal sizes that can be used for workshops. The below table describes the setup of the network in the four partitions. It provides essential information for the workshop with respect to the IP addresses and the services running at that IP address. Partition 1 Partition 2 Partition 3 Partition 4 Service Comment 10.38.x.1 10.38.x.65 10.38.x.129 10.38.x.193 Gateway 10.38.x.5 10.38.x.69 10.38.x.133 10.38.x.197 AHV Host 10.38.x.6 10.38.x.70 10.38.x.134 10.38.x.198 CVM IP 10.38.x.7 10.38.x.71 10.38.x.135 10.38.x.199 Cluster IP 10.38.x.8 10.38.x.72 10.38.x.136 10.38.x.200 Data Services IP 10.38.x.9 10.38.x.73 10.38.x.137 10.38.x.201 Prism Central IP 10.38.x.11 10.38.x.75 10.38.x.139 10.38.x.203 AutoDC IP(DC) 10.38.x.32-37 10.38.x.96-101 10.38.x.160-165 10.38.x.224-229 Objects 1 10.38.x.38-58 10.38.x.102-122 10.38.x.166-186 10.38.x.230-250 Primary 6 Free IPs for static assignment","title":"Single Node HPOC Clusters"},{"location":"#credentials","text":"Note The Cluster Password is unique to each cluster and will be provided by the leader of the Workshop. Credential Username Password Prism Element admin Cluster Password Prism Central admin Cluster Password Controller VM nutanix Cluster Password Prism Central VM nutanix Cluster Password Each cluster has a dedicated domain controller VM, DC , responsible for providing AD services for the NTNXLAB.local domain. The domain is populated with the following Users and Groups: Group Username(s) Password Administrators Administrator nutanix/4u SSP Admins adminuser01-adminuser25 nutanix/4u SSP Developers devuser01-devuser25 nutanix/4u SSP Consumers consumer01-consumer25 nutanix/4u SSP Operators operator01-operator25 nutanix/4u SSP Custom custom01-custom25 nutanix/4u Bootcamp Users user01-user25 nutanix/4u","title":"Credentials"},{"location":"#access-instructions","text":"The Nutanix Hosted POC environment can be accessed a number of different ways:","title":"Access Instructions"},{"location":"#lab-access-user-credentials","text":"PHX Based Clusters: Username: PHX-POCxxx-User01 (up to PHX-POCxxx-User20), Password: Provided by Instructor RTP Based Clusters: Username: RTP-POCxxx-User01 (up to RTP-POCxxx-User20), Password: Provided by Instructor","title":"Lab Access User Credentials"},{"location":"#frame-vdi","text":"Login to: https://console.nutanix.com/x/labs Nutanix Employees - Use your NUTANIXDC credentials Non-Employees - Use Lab Access User Credentials","title":"Frame VDI"},{"location":"#parallels-vdi","text":"PHX Based Clusters Login to: https://xld-uswest1.nutanix.com RTP Based Clusters Login to: https://xld-useast1.nutanix.com Nutanix Employees - Use your NUTANIXDC credentials Non-Employees - Use Lab Access User Credentials","title":"Parallels VDI"},{"location":"#employee-pulse-secure-vpn","text":"Download the client: PHX Based Clusters Login to: https://xld-uswest1.nutanix.com RTP Based Clusters Login to: https://xld-useast1.nutanix.com Nutanix Employees - Use your NUTANIXDC credentials Non-Employees - Use Lab Access User Credentials Install the client. In Pulse Secure Client, Add a connection: For PHX: Type - Policy Secure (UAC) or Connection Server Name - X-Labs - PHX Server URL - xlv-uswest1.nutanix.com For RTP: Type - Policy Secure (UAC) or Connection Server Name - X-Labs - RTP Server URL - xlv-useast1.nutanix.com","title":"Employee Pulse Secure VPN"},{"location":"#nutanix-version-info","text":"AOS 5.20.3 Prism Central pc.2022.4.x","title":"Nutanix Version Info"},{"location":"diyfoundation/diyfoundation/","text":"Overview Note Estimated time to complete: 60 Minutes Foundation is used to automate the installation of the hypervisor and Controller VM on one or more nodes. In this exercise you will practice imaging a physical cluster with Foundation. In order to keep the lab self-contained, you will create a single node cluster on which you will deploy your Foundation VM. That Foundation instance will be used to image and create a cluster from the remaining 3 nodes in the Block. In following steps, you may replace xx with your assigned cluster ID DIY Environment A Hosted POC reservation provides a fully imaged cluster consisting of 4 nodes. To keep the lab self-contained within a single, physical block, you will: Destroy the existing cluster Confirm the number of SSDs Create a single node cluster using Node D Install the Foundation VM on Node D Use Foundation VM to image Nodes A, B, and C and create a 3 node cluster Using an SSH client, connect to the Node D CVM IP 10.42.xx.32 in your assigned block using the following credentials: Username - nutanix Password - check password in RX Login to the console of NodeD CVM ssh nutanix@10.42.xx.32 # password: <check password in RX> Execute the following commands to power off any running VMs on the cluster, stop cluster services, and destroy the existing cluster: cluster stop # Enter 'I agree' when prompted to proceed cluster destroy # Enter 'Y' when prompted to proceed Confirm the Number of SSDs Confirm the number of SSD in your node D. This will determine which command we will use in the next section. It is likely that all nodes in HPOC cluster will have similar SSD and HDD combination. lsscsi # Example output here [ 0 :0:0:0 ] disk ATA INTEL SSDSC2BX80 0140 /dev/sda # << SSD 1 [ 0 :0:1:0 ] disk ATA INTEL SSDSC2BX80 0140 /dev/sdb # << SSD 2 [ 0 :0:2:0 ] disk ATA ST91000640NS SN03 /dev/sdc [ 0 :0:3:0 ] disk ATA ST91000640NS SN03 /dev/sdd [ 0 :0:4:0 ] disk ATA ST91000640NS SN03 /dev/sde [ 0 :0:5:0 ] disk ATA ST91000640NS SN03 /dev/sdf [ 2 :0:0:0 ] cd/dvd QEMU QEMU DVD-ROM 2 .5+ /dev/sr0 # this output shows that your node D has 2 SSDs lsscsi # Example output here [ 0 :0:0:0 ] disk ATA INTEL SSDSC2BX80 0140 /dev/sda # << SSD 1 [ 0 :0:2:0 ] disk ATA ST91000640NS SN03 /dev/sdc [ 0 :0:3:0 ] disk ATA ST91000640NS SN03 /dev/sdd [ 0 :0:4:0 ] disk ATA ST91000640NS SN03 /dev/sde [ 0 :0:5:0 ] disk ATA ST91000640NS SN03 /dev/sdf [ 2 :0:0:0 ] cd/dvd QEMU QEMU DVD-ROM 2 .5+ /dev/sr0 # this output shows that your node D has 1 SSD After confirming the number of SSDs choose the appropriate cluster formation script in the next section. Create Node D Cluster Remaining in SSH client, access Node-D CVM and execute following commands Login to the console of Node D CVM ssh nutanix@10.42.xx.32 # password: <check password in RX> Confirm if your hardware nodes have 1 or more SSD. 2 SSDs are required to privide RF2 redundancy factor in a Nutanix cluster. For the purpose of our lab, since we are creating 1 node cluster, we are good to have RF1 as a redundancy factor. If there 2 or more SSDs on your nodes, use this command to create a 1 node cluster cluster -s 10 .42.xx.32 create # Enter 'Y' when prompted to proceed If there is only 1 SSD in your nodes, use this command to create a 1 node cluster cluster -s 10 .42.xx.32 --redundancy_factor = 1 create # Enter 'Y' when prompted to proceed After the single node cluster is formed, run the following commands to configure it Note The above command will create a cluster from a single node using RF1, offering no redundancy to recover from hardware failure. This configuration is being used for non-production, instructional purposes and should NEVER be used for a customer deployment (unless the hosted application stores two copies of the same data e.g. Splunk). This should be agreed with the customer. After the cluster is created, Prism will reflect Critical Health status due to lack of redundancy. ncli cluster edit-params new-name = POCxx-D ncli cluster add-to-name-servers servers = 10 .42.196.10 ncli user reset-password user-name = 'admin' password = <check password in RX> Install Foundation VM Open https://<Node D CVM IP:9440 (https://10.42.xx.32:9440) in your browser and log in with the following credentials: Username - admin Password - check password in RX Accept the EULA and Pulse prompts. In Prism > Storage > Table > Storage Pool , select default storage pool and click update, then rename it to SP01 Check if there is a container named Images , if not, Click + Storage Container to create a new container named Images Go to configuration (Settings) page and navigate to Image Configuration , click +Upload Image Fill out the following fields and click Save : Name - Foundation Image Type - Disk Storage Container Images Select From URL Image Source - http://10.42.194.11/images/Foundation/Foundation_VM-5.2-disk-0.qcow2 Note At the time of writing, Foundation 5.2 is the latest available version. The URL for the latest Foundation VM QCOW2 image can be downloaded from the Nutanix Portal . Unless otherwise directed by support, always use the latest version of Foundation in a field installation. For the puposes of this lab, the Foundation VM image is stored in a HPOC file server Go to configuration page and navigate to Network Config Before creating the VM, we must first create a virtual network to assign to the Foundation VM. The network will use the Native VLAN assigned to the physical uplinks for all 4 nodes in the block. In the Prism Element UI click > Network Configuration > Networks > Create Network Fill out the following fields: Name - Primary VLAN ID - 0 Enable IP address management - leave it unselected Click on Save In Prism > VM > Table and click + Create VM . Fill out the following fields Name - Foundation vCPU(s) - 2 Number of Cores per vCPU - 1 Memory - 8 Gi Select + Add New Disk Operation - Clone from Image Service Image - Foundation Select Add Select Add New NIC VLAN Name - Primary Select Add Once NIC is added, you should see the NIC configuration in the VM create window as shown here Click on Save Config Foundation VM Select your Foundation VM and click Power on . Once the VM has started, click Launch Console . Once the VM has finished booting, return to Prism element and note down the IP address of the Foundation VM. Prism Element > VM > Table > Foundation VM > NICs Caution The IP address is received from the Primary network default DHCP pool. Your Foundation VM\\'s IP address will be different. Foundation Node ABC cluster Note We will do this section of the lab from your desktop (Windows or Mac) computer. This is the fastest way as remote consoles will be slow. By default, Foundation does not have any AOS or hypervisor images. You can download your desired AOS package from the Nutanix Portal . If downloading the AOS package within the Foundation VM, the .tar.gz package can also be moved to \\~/foundation/nos rather than uploaded to Foundation through the web UI. To shorten the lab time, we use command line to access foundation VM and download NOS binary to designated folder in it. Open a terminal in your desktop computer (Putty or Mac Terminal) and ssh to Foundation VM through foundation IP 10.42.xx.45 Login to the console of Foundation VM ssh -l nutanix <Foundation VM IP> # provide default password # example # ssh -l nutanix 10.42.xx.51 cd foundation cd nos curl -O http://10.42.194.11/images/AOS/5.20.3/nutanix_installer_package-release-euphrates-5.20.3-stable-x86_64.tar When you see 100% finish, AOS 5.20.3 package has been downloaded to ~/foundation/nos folder. From you desktop computer, open Google Chrome browser and navigate to Foundation VM's IP Access Foundation UI via any browser at http://<Foundation VM IP> Fill the following fields: Select your hardware platform : Autodetect Netmask of Every Host and CVM - 255.255.255.128 Gateway of Every Host and CVM - 10.42.xx.1 Gateway of Every IPMI - 10.42.xx.1 Netmask of Every IPMI - 255.255.255.128 Under Double-check this installer\\'s networking step Skip this Validation - selected In new foundation page, Tools menu choose Remove Unselected Rows to clear all auto discovered nodes Click Add nodes manually Fill in block information, fill in the following information: Number of blocks - 1 Number of nodes - 3 How should these nodes be reached? - choose I will provide the IPMI's MACs Click Add Tip Foundation will automatically discover any hosts in the same IPv6 Link Local broadcast domain that is not already part of a cluster. When transferring POC assets in the field, it's not uncommon to receive a cluster that wasn't properly destroyed at the conclusion of the previous POC. In that case, the nodes are already part of existing clusters and will not be discovered. In this lab, we choose manually specify the MAC address instead in order to practice as the real world. Info There are at least 2 methods to get MAC address remotely. Method.1: Identify IPMI MAC Address (BMC MAC address) of Nodes (A, B, and C) by accessing IPMI IP in a browser for each node Method.2 Identify IPMI MAC Address of Nodes (A, B, C) by login AHV host with User: root, Password: default for each node and using the following commands: ssh -l root <IP address of Host/Hypervisor> root@POC79-A ~# ipmitool lan print | grep \"MAC Address\" # output here MAC Address : 0c:c4:7a:3c:c9:ad # repeat for nodes B and C for unique IPMI MAC addresses Access Node A IPMI through IP 10.42.xx.33 with ADMIN/ADMIN Record your NODE A/B/C BMC MAC address (in above example , it is ac:1f:6b:1e:95:eb ) Doing the same with your other 2 nodes B/C, access Node B and C IPMI through IP 10.42.xx.34/35 with ADMIN/ADMIN, record all 3 BMC MAC addresses. Click Tools and select Range Autofill from the drop down list Replacing the octet(s) that correspond to your HPOC network, fill out the top row fields with the following details: IPMI MAC - the three your just recorded down IPMI IP - 10.42.xx.33 Hypervisor IP - 10.42.xx.25 CVM IP - 10.42.xx.29 HOSTNAME OF HOST -- POCxx-A Click Next In the Cluster page, fill the following details: Cluster Name - POCxx-ABC Timezone of Every Hypervisor and CVM - America/Phoenix Cluster Redundancy Factor - RF2 Cluster Virtual IP - 10.42.xx.37 NTP Servers of Every Hypervisor and CVM - 0.pool.ntp.org,1.pool.ntp.org,2.pool.ntp.org,3.pool.ntp.org DNS Servers of Every Hypervisor and CVM - 10.42.196.10 vRAM Allocation for Every CVM, in Gigabytes - 32 Click Next Select an AOS installer - Select your uploaded (through command line in previous steps) nutanix_installer_package-release-*.tar.gz file Arguments to the AOS Installer (Optional) - leave blank Click Next Fill out the following fields and click Next : Select a hypervisor installer - AHV, AHV installer bundled inside the AOS installer Tip Every AOS release contains a version of AHV bundle with that release. Click Next Enter the existing IPMI credentials as ADMIN and ADMIN for all three nodes. Note that this will be different in the field. Click Start Confirm that the installer will be active by clicking on Won't Sleep In the Warning of Data Loss Possibility window, click on Ignore and Re-image Foundation will run a couple of tests to make sure all the configuration details you have provided are correct and then direct you the installation progress page. Click the Log link to view the realtime log output from your node. When all CVMs are ready, Foundation initiates the cluster creation process. Monitor the foundation process until completion Once Foundation finishes successully, either click on Click here link as shown above or open https://*<Cluster Virtual IP >*:9440 (10.42.xx.37)in your browser Log in with the following credentials: Username - admin Password - default Change the Password - check password in RX Takeaways You have successfully prepared your environment in a single operation called Foundation: Installed Hypervisor (AHV) - This can also be ESXi or Hyper V Installed CVM (AOS) Distributed File System (Data Plane) Prism Element (Control Plane)","title":"Foundation"},{"location":"diyfoundation/diyfoundation/#overview","text":"Note Estimated time to complete: 60 Minutes Foundation is used to automate the installation of the hypervisor and Controller VM on one or more nodes. In this exercise you will practice imaging a physical cluster with Foundation. In order to keep the lab self-contained, you will create a single node cluster on which you will deploy your Foundation VM. That Foundation instance will be used to image and create a cluster from the remaining 3 nodes in the Block. In following steps, you may replace xx with your assigned cluster ID","title":"Overview"},{"location":"diyfoundation/diyfoundation/#diy-environment","text":"A Hosted POC reservation provides a fully imaged cluster consisting of 4 nodes. To keep the lab self-contained within a single, physical block, you will: Destroy the existing cluster Confirm the number of SSDs Create a single node cluster using Node D Install the Foundation VM on Node D Use Foundation VM to image Nodes A, B, and C and create a 3 node cluster Using an SSH client, connect to the Node D CVM IP 10.42.xx.32 in your assigned block using the following credentials: Username - nutanix Password - check password in RX Login to the console of NodeD CVM ssh nutanix@10.42.xx.32 # password: <check password in RX> Execute the following commands to power off any running VMs on the cluster, stop cluster services, and destroy the existing cluster: cluster stop # Enter 'I agree' when prompted to proceed cluster destroy # Enter 'Y' when prompted to proceed","title":"DIY Environment"},{"location":"diyfoundation/diyfoundation/#confirm-the-number-of-ssds","text":"Confirm the number of SSD in your node D. This will determine which command we will use in the next section. It is likely that all nodes in HPOC cluster will have similar SSD and HDD combination. lsscsi # Example output here [ 0 :0:0:0 ] disk ATA INTEL SSDSC2BX80 0140 /dev/sda # << SSD 1 [ 0 :0:1:0 ] disk ATA INTEL SSDSC2BX80 0140 /dev/sdb # << SSD 2 [ 0 :0:2:0 ] disk ATA ST91000640NS SN03 /dev/sdc [ 0 :0:3:0 ] disk ATA ST91000640NS SN03 /dev/sdd [ 0 :0:4:0 ] disk ATA ST91000640NS SN03 /dev/sde [ 0 :0:5:0 ] disk ATA ST91000640NS SN03 /dev/sdf [ 2 :0:0:0 ] cd/dvd QEMU QEMU DVD-ROM 2 .5+ /dev/sr0 # this output shows that your node D has 2 SSDs lsscsi # Example output here [ 0 :0:0:0 ] disk ATA INTEL SSDSC2BX80 0140 /dev/sda # << SSD 1 [ 0 :0:2:0 ] disk ATA ST91000640NS SN03 /dev/sdc [ 0 :0:3:0 ] disk ATA ST91000640NS SN03 /dev/sdd [ 0 :0:4:0 ] disk ATA ST91000640NS SN03 /dev/sde [ 0 :0:5:0 ] disk ATA ST91000640NS SN03 /dev/sdf [ 2 :0:0:0 ] cd/dvd QEMU QEMU DVD-ROM 2 .5+ /dev/sr0 # this output shows that your node D has 1 SSD After confirming the number of SSDs choose the appropriate cluster formation script in the next section.","title":"Confirm the Number of SSDs"},{"location":"diyfoundation/diyfoundation/#create-node-d-cluster","text":"Remaining in SSH client, access Node-D CVM and execute following commands Login to the console of Node D CVM ssh nutanix@10.42.xx.32 # password: <check password in RX> Confirm if your hardware nodes have 1 or more SSD. 2 SSDs are required to privide RF2 redundancy factor in a Nutanix cluster. For the purpose of our lab, since we are creating 1 node cluster, we are good to have RF1 as a redundancy factor. If there 2 or more SSDs on your nodes, use this command to create a 1 node cluster cluster -s 10 .42.xx.32 create # Enter 'Y' when prompted to proceed If there is only 1 SSD in your nodes, use this command to create a 1 node cluster cluster -s 10 .42.xx.32 --redundancy_factor = 1 create # Enter 'Y' when prompted to proceed After the single node cluster is formed, run the following commands to configure it Note The above command will create a cluster from a single node using RF1, offering no redundancy to recover from hardware failure. This configuration is being used for non-production, instructional purposes and should NEVER be used for a customer deployment (unless the hosted application stores two copies of the same data e.g. Splunk). This should be agreed with the customer. After the cluster is created, Prism will reflect Critical Health status due to lack of redundancy. ncli cluster edit-params new-name = POCxx-D ncli cluster add-to-name-servers servers = 10 .42.196.10 ncli user reset-password user-name = 'admin' password = <check password in RX>","title":"Create Node D Cluster"},{"location":"diyfoundation/diyfoundation/#install-foundation-vm","text":"Open https://<Node D CVM IP:9440 (https://10.42.xx.32:9440) in your browser and log in with the following credentials: Username - admin Password - check password in RX Accept the EULA and Pulse prompts. In Prism > Storage > Table > Storage Pool , select default storage pool and click update, then rename it to SP01 Check if there is a container named Images , if not, Click + Storage Container to create a new container named Images Go to configuration (Settings) page and navigate to Image Configuration , click +Upload Image Fill out the following fields and click Save : Name - Foundation Image Type - Disk Storage Container Images Select From URL Image Source - http://10.42.194.11/images/Foundation/Foundation_VM-5.2-disk-0.qcow2 Note At the time of writing, Foundation 5.2 is the latest available version. The URL for the latest Foundation VM QCOW2 image can be downloaded from the Nutanix Portal . Unless otherwise directed by support, always use the latest version of Foundation in a field installation. For the puposes of this lab, the Foundation VM image is stored in a HPOC file server Go to configuration page and navigate to Network Config Before creating the VM, we must first create a virtual network to assign to the Foundation VM. The network will use the Native VLAN assigned to the physical uplinks for all 4 nodes in the block. In the Prism Element UI click > Network Configuration > Networks > Create Network Fill out the following fields: Name - Primary VLAN ID - 0 Enable IP address management - leave it unselected Click on Save In Prism > VM > Table and click + Create VM . Fill out the following fields Name - Foundation vCPU(s) - 2 Number of Cores per vCPU - 1 Memory - 8 Gi Select + Add New Disk Operation - Clone from Image Service Image - Foundation Select Add Select Add New NIC VLAN Name - Primary Select Add Once NIC is added, you should see the NIC configuration in the VM create window as shown here Click on Save","title":"Install Foundation VM"},{"location":"diyfoundation/diyfoundation/#config-foundation-vm","text":"Select your Foundation VM and click Power on . Once the VM has started, click Launch Console . Once the VM has finished booting, return to Prism element and note down the IP address of the Foundation VM. Prism Element > VM > Table > Foundation VM > NICs Caution The IP address is received from the Primary network default DHCP pool. Your Foundation VM\\'s IP address will be different.","title":"Config Foundation VM"},{"location":"diyfoundation/diyfoundation/#foundation-node-abc-cluster","text":"Note We will do this section of the lab from your desktop (Windows or Mac) computer. This is the fastest way as remote consoles will be slow. By default, Foundation does not have any AOS or hypervisor images. You can download your desired AOS package from the Nutanix Portal . If downloading the AOS package within the Foundation VM, the .tar.gz package can also be moved to \\~/foundation/nos rather than uploaded to Foundation through the web UI. To shorten the lab time, we use command line to access foundation VM and download NOS binary to designated folder in it. Open a terminal in your desktop computer (Putty or Mac Terminal) and ssh to Foundation VM through foundation IP 10.42.xx.45 Login to the console of Foundation VM ssh -l nutanix <Foundation VM IP> # provide default password # example # ssh -l nutanix 10.42.xx.51 cd foundation cd nos curl -O http://10.42.194.11/images/AOS/5.20.3/nutanix_installer_package-release-euphrates-5.20.3-stable-x86_64.tar When you see 100% finish, AOS 5.20.3 package has been downloaded to ~/foundation/nos folder. From you desktop computer, open Google Chrome browser and navigate to Foundation VM's IP Access Foundation UI via any browser at http://<Foundation VM IP> Fill the following fields: Select your hardware platform : Autodetect Netmask of Every Host and CVM - 255.255.255.128 Gateway of Every Host and CVM - 10.42.xx.1 Gateway of Every IPMI - 10.42.xx.1 Netmask of Every IPMI - 255.255.255.128 Under Double-check this installer\\'s networking step Skip this Validation - selected In new foundation page, Tools menu choose Remove Unselected Rows to clear all auto discovered nodes Click Add nodes manually Fill in block information, fill in the following information: Number of blocks - 1 Number of nodes - 3 How should these nodes be reached? - choose I will provide the IPMI's MACs Click Add Tip Foundation will automatically discover any hosts in the same IPv6 Link Local broadcast domain that is not already part of a cluster. When transferring POC assets in the field, it's not uncommon to receive a cluster that wasn't properly destroyed at the conclusion of the previous POC. In that case, the nodes are already part of existing clusters and will not be discovered. In this lab, we choose manually specify the MAC address instead in order to practice as the real world. Info There are at least 2 methods to get MAC address remotely. Method.1: Identify IPMI MAC Address (BMC MAC address) of Nodes (A, B, and C) by accessing IPMI IP in a browser for each node Method.2 Identify IPMI MAC Address of Nodes (A, B, C) by login AHV host with User: root, Password: default for each node and using the following commands: ssh -l root <IP address of Host/Hypervisor> root@POC79-A ~# ipmitool lan print | grep \"MAC Address\" # output here MAC Address : 0c:c4:7a:3c:c9:ad # repeat for nodes B and C for unique IPMI MAC addresses Access Node A IPMI through IP 10.42.xx.33 with ADMIN/ADMIN Record your NODE A/B/C BMC MAC address (in above example , it is ac:1f:6b:1e:95:eb ) Doing the same with your other 2 nodes B/C, access Node B and C IPMI through IP 10.42.xx.34/35 with ADMIN/ADMIN, record all 3 BMC MAC addresses. Click Tools and select Range Autofill from the drop down list Replacing the octet(s) that correspond to your HPOC network, fill out the top row fields with the following details: IPMI MAC - the three your just recorded down IPMI IP - 10.42.xx.33 Hypervisor IP - 10.42.xx.25 CVM IP - 10.42.xx.29 HOSTNAME OF HOST -- POCxx-A Click Next In the Cluster page, fill the following details: Cluster Name - POCxx-ABC Timezone of Every Hypervisor and CVM - America/Phoenix Cluster Redundancy Factor - RF2 Cluster Virtual IP - 10.42.xx.37 NTP Servers of Every Hypervisor and CVM - 0.pool.ntp.org,1.pool.ntp.org,2.pool.ntp.org,3.pool.ntp.org DNS Servers of Every Hypervisor and CVM - 10.42.196.10 vRAM Allocation for Every CVM, in Gigabytes - 32 Click Next Select an AOS installer - Select your uploaded (through command line in previous steps) nutanix_installer_package-release-*.tar.gz file Arguments to the AOS Installer (Optional) - leave blank Click Next Fill out the following fields and click Next : Select a hypervisor installer - AHV, AHV installer bundled inside the AOS installer Tip Every AOS release contains a version of AHV bundle with that release. Click Next Enter the existing IPMI credentials as ADMIN and ADMIN for all three nodes. Note that this will be different in the field. Click Start Confirm that the installer will be active by clicking on Won't Sleep In the Warning of Data Loss Possibility window, click on Ignore and Re-image Foundation will run a couple of tests to make sure all the configuration details you have provided are correct and then direct you the installation progress page. Click the Log link to view the realtime log output from your node. When all CVMs are ready, Foundation initiates the cluster creation process. Monitor the foundation process until completion Once Foundation finishes successully, either click on Click here link as shown above or open https://*<Cluster Virtual IP >*:9440 (10.42.xx.37)in your browser Log in with the following credentials: Username - admin Password - default Change the Password - check password in RX","title":"Foundation Node ABC cluster"},{"location":"diyfoundation/diyfoundation/#takeaways","text":"You have successfully prepared your environment in a single operation called Foundation: Installed Hypervisor (AHV) - This can also be ESXi or Hyper V Installed CVM (AOS) Distributed File System (Data Plane) Prism Element (Control Plane)","title":"Takeaways"},{"location":"prism_central_deploy/prism_central_deploy/","text":"Overview Info Estimated time to complete: 30 Minutes This lab will introduce Prism Central's(PC) One-Click deploy process Create Primary and Secondary networks Alert The Primary network is for PC and other VMs deployment, the Secondary network is requried in X-Ray lab Open https://POCxx-ABC Cluster IP*:9440 (https://10.42.xx.37:9440) in your browser and log in with the following credentials: Username - admin Password - check password in RX In the Prism Element UI click > Network Configuration > Networks > Create Network Fill out the following fields: Name - Primary Virtual Switch - vs0 VLAN ID - 0 Enable IP address management - leave it unselected Click Save Create the second network by clicking on + Create Network with the following details: Name - Secondary Virtual Switch - vs0 VLAN ID - HPOC Cluster ID 1 (e.g. for PHX-POC079 , VLAN ID would be 791 ) Enable IP address management - leave it unselected Click Save You should see two networks as shown here Prism Central Deploy Navigate to Home page and click Register or create new in Prism Central widget. Choose the first Deploy option. Download the latest version and click Deploy 1-VM PC Fill out the following fields: VM Name - PC Select A Container - SelfServiceContainer VM Sizing - SMALL - (UP TO 2500 VMs) In Network config, fill our the following details (XX here is your POC number) AHV Network - Primary IP Address - 10.42.XX.39 Subnet Mask - 255.255.255.128 Default Gateway - 10.42.XX.1 DNS Address(Es) - 10.42.196.10 Click Deploy Note The deployment will take about 30 mins, you can go to next lab sessions while waiting. After Prism Central VM is successfully deployed, open https://*PC VM IP*:9440 (https://10.42.xx.39:9440) in your browser and log in with the following credentials: When the deployment finishes, browse to your Prism Central IP address (e.g. 10.42.XX.39) with the following details: Username - admin Password - default with capital N change password to - check password in RX Test if you can login Prism Central with the new password Accept EULA if displayed Prism Central Registration Go back to POCxx-ABC Cluster (https://10.42.xx.37:9440) Navigate to Home page and click cluster name POCxx-ABC and provide a cluster data service ip 10.42.xx.38 Click Register or create new in Prism Central widget. Choose the second Connect option. Click Next Fill out the following fields, leave others as default and click Connect : Prism Central IP - 10.42.xx.39 Port - 9440 Username - admin Password - check password in RX You will see an OK with PC\\'s IP in Prism Central widget. You have successully registered Prism Element to be managed your Prism Central. Note Once the Prism Element registration is complete, several management features on Prism Element will be Read-Only mode but fully available in Prism Central.","title":"Prism Central Deployment"},{"location":"prism_central_deploy/prism_central_deploy/#overview","text":"Info Estimated time to complete: 30 Minutes This lab will introduce Prism Central's(PC) One-Click deploy process","title":"Overview"},{"location":"prism_central_deploy/prism_central_deploy/#create-primary-and-secondary-networks","text":"Alert The Primary network is for PC and other VMs deployment, the Secondary network is requried in X-Ray lab Open https://POCxx-ABC Cluster IP*:9440 (https://10.42.xx.37:9440) in your browser and log in with the following credentials: Username - admin Password - check password in RX In the Prism Element UI click > Network Configuration > Networks > Create Network Fill out the following fields: Name - Primary Virtual Switch - vs0 VLAN ID - 0 Enable IP address management - leave it unselected Click Save Create the second network by clicking on + Create Network with the following details: Name - Secondary Virtual Switch - vs0 VLAN ID - HPOC Cluster ID 1 (e.g. for PHX-POC079 , VLAN ID would be 791 ) Enable IP address management - leave it unselected Click Save You should see two networks as shown here","title":"Create Primary and Secondary networks"},{"location":"prism_central_deploy/prism_central_deploy/#prism-central-deploy","text":"Navigate to Home page and click Register or create new in Prism Central widget. Choose the first Deploy option. Download the latest version and click Deploy 1-VM PC Fill out the following fields: VM Name - PC Select A Container - SelfServiceContainer VM Sizing - SMALL - (UP TO 2500 VMs) In Network config, fill our the following details (XX here is your POC number) AHV Network - Primary IP Address - 10.42.XX.39 Subnet Mask - 255.255.255.128 Default Gateway - 10.42.XX.1 DNS Address(Es) - 10.42.196.10 Click Deploy Note The deployment will take about 30 mins, you can go to next lab sessions while waiting. After Prism Central VM is successfully deployed, open https://*PC VM IP*:9440 (https://10.42.xx.39:9440) in your browser and log in with the following credentials: When the deployment finishes, browse to your Prism Central IP address (e.g. 10.42.XX.39) with the following details: Username - admin Password - default with capital N change password to - check password in RX Test if you can login Prism Central with the new password Accept EULA if displayed","title":"Prism Central Deploy"},{"location":"prism_central_deploy/prism_central_deploy/#prism-central-registration","text":"Go back to POCxx-ABC Cluster (https://10.42.xx.37:9440) Navigate to Home page and click cluster name POCxx-ABC and provide a cluster data service ip 10.42.xx.38 Click Register or create new in Prism Central widget. Choose the second Connect option. Click Next Fill out the following fields, leave others as default and click Connect : Prism Central IP - 10.42.xx.39 Port - 9440 Username - admin Password - check password in RX You will see an OK with PC\\'s IP in Prism Central widget. You have successully registered Prism Element to be managed your Prism Central. Note Once the Prism Element registration is complete, several management features on Prism Element will be Read-Only mode but fully available in Prism Central.","title":"Prism Central Registration"},{"location":"taskman/taskman/","text":"The estimated time to complete this lab is 20 minutes. Overview This exercise walks you through importing and launching a Calm blueprint to deploy a simple Task Manager application used in Day2's Flow labs. You do not need to complete this exercise unless directed to do so as staging for another lab. Enabling App Management Open https:// \\<Prism-Central-IP> :9440/ in a browser and log in. From the navigation bar, select Service > Calm Click Enable . Select Enable App Management and click Save . ::: note ::: title Note ::: Nutanix Calm is a separately licensed product that can be used with Acropolis Starter, Pro, or Ultimate editions. Each Prism Central instance can manage up to 25 VMs for free before additional licensing is required. ::: You should get verification that Calm is enabling, which will take 5 to 10 minutes. Creating A Project Projects are the logical construct that integrate Calm with Nutanix\\'s native Self-Service Portal (SSP) capabilities, allowing an administrator to assign both infrastructure resources and the roles/permissions of Active Directory users/groups to specific Blueprints and Applications. Click default in the project list Under Infrastructure , fill out the following fields and click comfirm : - Select which resources you want this project to consume - Nutanix - AHV Cluster - \\<POCxx-ABC> - Under Network , select the Primary and if available, the Secondary networks. Select star {.interpreted-text role=\"fa\"} for the Primary network to make it the default virtual network for VMs in the default project. Click Save . Verifying the Default Project In Prism Central , select bars {.interpreted-text role=\"fa\"} > Services > Calm . Click Projects in the left hand toolbar and select the default project. ::: note ::: title Note ::: Mousing over an icon will display its title. ::: Under AHV Cluster verify your assigned cluster is selected from the drop-down list, otherwise select it. Under Network , verify the Primary and Secondary networks are selected and the Primary network is the default. Otherwise, make the selections as shown below. If changes were made, click Save . Importing the Blueprint Right-click on this link <TaskManager.json> {.interpreted-text role=\"download\"} and Save Link As... to download the blueprint for the example application used in this exercise. Click Blueprints in the left hand toolbar to view available Calm blueprints. Click Upload Blueprint and select the TaskManager.json file previously downloaded. Fill out the following fields: Blueprint Name - Initials -TaskManager Project - default Click Upload . ::: note ::: title Note ::: If you receive an error trying to upload the blueprint, refresh your browser and try again. ::: Configuring the Blueprint Before you can launch the blueprint, you must first provide specify the information not stored in exported Calm blueprints, including credentials. In the Application Profile pane on the right, fill out the following field: Mysql_password - nutanix/4u Select the WinClient service and in the pane on the right, under the VM tab, ensure the Image is set to the Windows10 disk image as shown below. Under Network Adapters (NICs) , ensure that NIC 1 is set to Primary as shown below. Select the WebServer , HAProxy , and MySQL services and ensure each has NIC 1 set to Primary . Click Save . Click Credentials . Expand the CENTOS credential by clicking its name. Copy and paste the following key into the SSH Private Key field: -----BEGIN RSA PRIVATE KEY----- MIIEowIBAAKCAQEAii7qFDhVadLx5lULAG/ooCUTA/ATSmXbArs+GdHxbUWd/bNG ZCXnaQ2L1mSVVGDxfTbSaTJ3En3tVlMtD2RjZPdhqWESCaoj2kXLYSiNDS9qz3SK 6h822je/f9O9CzCTrw2XGhnDVwmNraUvO5wmQObCDthTXc72PcBOd6oa4ENsnuY9 HtiETg29TZXgCYPFXipLBHSZYkBmGgccAeY9dq5ywiywBJLuoSovXkkRJk3cd7Gy hCRIwYzqfdgSmiAMYgJLrz/UuLxatPqXts2D8v1xqR9EPNZNzgd4QHK4of1lqsNR uz2SxkwqLcXSw0mGcAL8mIwVpzhPzwmENC5OrwIBJQKCAQB++q2WCkCmbtByyrAp 6ktiukjTL6MGGGhjX/PgYA5IvINX1SvtU0NZnb7FAntiSz7GFrODQyFPQ0jL3bq0 MrwzRDA6x+cPzMb/7RvBEIGdadfFjbAVaMqfAsul5SpBokKFLxU6lDb2CMdhS67c 1K2Hv0qKLpHL0vAdEZQ2nFAMWETvVMzl0o1dQmyGzA0GTY8VYdCRsUbwNgvFMvBj 8T/svzjpASDifa7IXlGaLrXfCH584zt7y+qjJ05O1G0NFslQ9n2wi7F93N8rHxgl JDE4OhfyaDyLL1UdBlBpjYPSUbX7D5NExLggWEVFEwx4JRaK6+aDdFDKbSBIidHf h45NAoGBANjANRKLBtcxmW4foK5ILTuFkOaowqj+2AIgT1ezCVpErHDFg0bkuvDk QVdsAJRX5//luSO30dI0OWWGjgmIUXD7iej0sjAPJjRAv8ai+MYyaLfkdqv1Oj5c oDC3KjmSdXTuWSYNvarsW+Uf2v7zlZlWesTnpV6gkZH3tX86iuiZAoGBAKM0mKX0 EjFkJH65Ym7gIED2CUyuFqq4WsCUD2RakpYZyIBKZGr8MRni3I4z6Hqm+rxVW6Dj uFGQe5GhgPvO23UG1Y6nm0VkYgZq81TraZc/oMzignSC95w7OsLaLn6qp32Fje1M Ez2Yn0T3dDcu1twY8OoDuvWx5LFMJ3NoRJaHAoGBAJ4rZP+xj17DVElxBo0EPK7k 7TKygDYhwDjnJSRSN0HfFg0agmQqXucjGuzEbyAkeN1Um9vLU+xrTHqEyIN/Jqxk hztKxzfTtBhK7M84p7M5iq+0jfMau8ykdOVHZAB/odHeXLrnbrr/gVQsAKw1NdDC kPCNXP/c9JrzB+c4juEVAoGBAJGPxmp/vTL4c5OebIxnCAKWP6VBUnyWliFhdYME rECvNkjoZ2ZWjKhijVw8Il+OAjlFNgwJXzP9Z0qJIAMuHa2QeUfhmFKlo4ku9LOF 2rdUbNJpKD5m+IRsLX1az4W6zLwPVRHp56WjzFJEfGiRjzMBfOxkMSBSjbLjDm3Z iUf7AoGBALjvtjapDwlEa5/CFvzOVGFq4L/OJTBEBGx/SA4HUc3TFTtlY2hvTDPZ dQr/JBzLBUjCOBVuUuH3uW7hGhW+DnlzrfbfJATaRR8Ht6VU651T+Gbrr8EqNpCP gmznERCNf9Kaxl/hlyV5dZBe/2LIK+/jLGNu9EJLoraaCBFshJKF -----END RSA PRIVATE KEY----- Expand the WIN_VM_CRED credential by clicking its name. Enter nutanix/4u as the Password . Click Save . Once the blueprint has been saved, click Back . Launching the Blueprint After the credentials have been provided, Publish , Download , and Launch are now available from the toolbar. Click Launch . Fill out the following fields: Name of the Application - Initials -TaskManager1 User_initials - Initials Click Create . You can monitor the status of your application deployment by clicking Applications and clicking your application\\'s name. Provisioning the complete application will take approximately 15 minutes. Proceed to the next section of the lab while the application is provisioning.","title":"Deploying Task Manager"},{"location":"taskman/taskman/#overview","text":"This exercise walks you through importing and launching a Calm blueprint to deploy a simple Task Manager application used in Day2's Flow labs. You do not need to complete this exercise unless directed to do so as staging for another lab.","title":"Overview"},{"location":"taskman/taskman/#enabling-app-management","text":"Open https:// \\<Prism-Central-IP> :9440/ in a browser and log in. From the navigation bar, select Service > Calm Click Enable . Select Enable App Management and click Save . ::: note ::: title Note ::: Nutanix Calm is a separately licensed product that can be used with Acropolis Starter, Pro, or Ultimate editions. Each Prism Central instance can manage up to 25 VMs for free before additional licensing is required. ::: You should get verification that Calm is enabling, which will take 5 to 10 minutes.","title":"Enabling App Management"},{"location":"taskman/taskman/#creating-a-project","text":"Projects are the logical construct that integrate Calm with Nutanix\\'s native Self-Service Portal (SSP) capabilities, allowing an administrator to assign both infrastructure resources and the roles/permissions of Active Directory users/groups to specific Blueprints and Applications. Click default in the project list Under Infrastructure , fill out the following fields and click comfirm : - Select which resources you want this project to consume - Nutanix - AHV Cluster - \\<POCxx-ABC> - Under Network , select the Primary and if available, the Secondary networks. Select star {.interpreted-text role=\"fa\"} for the Primary network to make it the default virtual network for VMs in the default project. Click Save .","title":"Creating A Project"},{"location":"taskman/taskman/#verifying-the-default-project","text":"In Prism Central , select bars {.interpreted-text role=\"fa\"} > Services > Calm . Click Projects in the left hand toolbar and select the default project. ::: note ::: title Note ::: Mousing over an icon will display its title. ::: Under AHV Cluster verify your assigned cluster is selected from the drop-down list, otherwise select it. Under Network , verify the Primary and Secondary networks are selected and the Primary network is the default. Otherwise, make the selections as shown below. If changes were made, click Save .","title":"Verifying the Default Project"},{"location":"taskman/taskman/#importing-the-blueprint","text":"Right-click on this link <TaskManager.json> {.interpreted-text role=\"download\"} and Save Link As... to download the blueprint for the example application used in this exercise. Click Blueprints in the left hand toolbar to view available Calm blueprints. Click Upload Blueprint and select the TaskManager.json file previously downloaded. Fill out the following fields: Blueprint Name - Initials -TaskManager Project - default Click Upload . ::: note ::: title Note ::: If you receive an error trying to upload the blueprint, refresh your browser and try again. :::","title":"Importing the Blueprint"},{"location":"taskman/taskman/#configuring-the-blueprint","text":"Before you can launch the blueprint, you must first provide specify the information not stored in exported Calm blueprints, including credentials. In the Application Profile pane on the right, fill out the following field: Mysql_password - nutanix/4u Select the WinClient service and in the pane on the right, under the VM tab, ensure the Image is set to the Windows10 disk image as shown below. Under Network Adapters (NICs) , ensure that NIC 1 is set to Primary as shown below. Select the WebServer , HAProxy , and MySQL services and ensure each has NIC 1 set to Primary . Click Save . Click Credentials . Expand the CENTOS credential by clicking its name. Copy and paste the following key into the SSH Private Key field: -----BEGIN RSA PRIVATE KEY----- MIIEowIBAAKCAQEAii7qFDhVadLx5lULAG/ooCUTA/ATSmXbArs+GdHxbUWd/bNG ZCXnaQ2L1mSVVGDxfTbSaTJ3En3tVlMtD2RjZPdhqWESCaoj2kXLYSiNDS9qz3SK 6h822je/f9O9CzCTrw2XGhnDVwmNraUvO5wmQObCDthTXc72PcBOd6oa4ENsnuY9 HtiETg29TZXgCYPFXipLBHSZYkBmGgccAeY9dq5ywiywBJLuoSovXkkRJk3cd7Gy hCRIwYzqfdgSmiAMYgJLrz/UuLxatPqXts2D8v1xqR9EPNZNzgd4QHK4of1lqsNR uz2SxkwqLcXSw0mGcAL8mIwVpzhPzwmENC5OrwIBJQKCAQB++q2WCkCmbtByyrAp 6ktiukjTL6MGGGhjX/PgYA5IvINX1SvtU0NZnb7FAntiSz7GFrODQyFPQ0jL3bq0 MrwzRDA6x+cPzMb/7RvBEIGdadfFjbAVaMqfAsul5SpBokKFLxU6lDb2CMdhS67c 1K2Hv0qKLpHL0vAdEZQ2nFAMWETvVMzl0o1dQmyGzA0GTY8VYdCRsUbwNgvFMvBj 8T/svzjpASDifa7IXlGaLrXfCH584zt7y+qjJ05O1G0NFslQ9n2wi7F93N8rHxgl JDE4OhfyaDyLL1UdBlBpjYPSUbX7D5NExLggWEVFEwx4JRaK6+aDdFDKbSBIidHf h45NAoGBANjANRKLBtcxmW4foK5ILTuFkOaowqj+2AIgT1ezCVpErHDFg0bkuvDk QVdsAJRX5//luSO30dI0OWWGjgmIUXD7iej0sjAPJjRAv8ai+MYyaLfkdqv1Oj5c oDC3KjmSdXTuWSYNvarsW+Uf2v7zlZlWesTnpV6gkZH3tX86iuiZAoGBAKM0mKX0 EjFkJH65Ym7gIED2CUyuFqq4WsCUD2RakpYZyIBKZGr8MRni3I4z6Hqm+rxVW6Dj uFGQe5GhgPvO23UG1Y6nm0VkYgZq81TraZc/oMzignSC95w7OsLaLn6qp32Fje1M Ez2Yn0T3dDcu1twY8OoDuvWx5LFMJ3NoRJaHAoGBAJ4rZP+xj17DVElxBo0EPK7k 7TKygDYhwDjnJSRSN0HfFg0agmQqXucjGuzEbyAkeN1Um9vLU+xrTHqEyIN/Jqxk hztKxzfTtBhK7M84p7M5iq+0jfMau8ykdOVHZAB/odHeXLrnbrr/gVQsAKw1NdDC kPCNXP/c9JrzB+c4juEVAoGBAJGPxmp/vTL4c5OebIxnCAKWP6VBUnyWliFhdYME rECvNkjoZ2ZWjKhijVw8Il+OAjlFNgwJXzP9Z0qJIAMuHa2QeUfhmFKlo4ku9LOF 2rdUbNJpKD5m+IRsLX1az4W6zLwPVRHp56WjzFJEfGiRjzMBfOxkMSBSjbLjDm3Z iUf7AoGBALjvtjapDwlEa5/CFvzOVGFq4L/OJTBEBGx/SA4HUc3TFTtlY2hvTDPZ dQr/JBzLBUjCOBVuUuH3uW7hGhW+DnlzrfbfJATaRR8Ht6VU651T+Gbrr8EqNpCP gmznERCNf9Kaxl/hlyV5dZBe/2LIK+/jLGNu9EJLoraaCBFshJKF -----END RSA PRIVATE KEY----- Expand the WIN_VM_CRED credential by clicking its name. Enter nutanix/4u as the Password . Click Save . Once the blueprint has been saved, click Back .","title":"Configuring the Blueprint"},{"location":"taskman/taskman/#launching-the-blueprint","text":"After the credentials have been provided, Publish , Download , and Launch are now available from the toolbar. Click Launch . Fill out the following fields: Name of the Application - Initials -TaskManager1 User_initials - Initials Click Create . You can monitor the status of your application deployment by clicking Applications and clicking your application\\'s name. Provisioning the complete application will take approximately 15 minutes. Proceed to the next section of the lab while the application is provisioning.","title":"Launching the Blueprint"},{"location":"tools_vms/linux_tools_vm/","text":"Overview This CentOS VM image will be staged with packages used to support multiple lab exercises. Deploy this VM on your assigned cluster if directed to do so as part of Lab Setup . <strong><font color=\"red\">Only deploy the VM once, it does not need to be cleaned up as part of any lab completion.</font></strong> Deploying CentOS In Prism Central > select bars {.interpreted-text role=\"fa\"} > Virtual Infrastructure > VMs , and click Create VM . Fill out the following fields: Name - Initials -Linux-ToolsVM Description - (Optional) Description for your VM. vCPU(s) - 1 Number of Cores per vCPU - 2 Memory - 2 GiB Select + Add New Disk : - Type - DISK - Operation - Clone from Image Service - Image - CentOS7.qcow2 - Select Add Select Add New NIC : - VLAN Name - Secondary - Select Add Click Save to create the VM. Power On the VM. Installing Tools Login to the VM via ssh or Console session, using the following credentials: Username - root password - nutanix/4u Install the software needed by running the following commands: yum update -y yum install -y ntp ntpdate unzip stress nodejs python-pip s3cmd awscli npm install -g request npm install -g express Configuring NTP Enable and configure NTP by running the following commands: systemctl start ntpd systemctl enable ntpd ntpdate -u -s 0 .pool.ntp.org 1 .pool.ntp.org 2 .pool.ntp.org 3 .pool.ntp.org systemctl restart ntpd Disabling Firewall and SELinux Disable the firewall and SELinux by running the following commands: systemctl disable firewalld systemctl stop firewalld setenforce 0 sed -i 's/enforcing/disabled/g' /etc/selinux/config /etc/selinux/config Installing Python If completing the apis {.interpreted-text role=\"ref\"} lab using the Linux Tools VM, install Python by running the following commands: yum -y install python36 python3.6 -m ensurepip yum -y install python36-setuptools pip install -U pip pip install boto3","title":"Linux Tools VM"},{"location":"tools_vms/linux_tools_vm/#overview","text":"This CentOS VM image will be staged with packages used to support multiple lab exercises. Deploy this VM on your assigned cluster if directed to do so as part of Lab Setup . <strong><font color=\"red\">Only deploy the VM once, it does not need to be cleaned up as part of any lab completion.</font></strong>","title":"Overview"},{"location":"tools_vms/linux_tools_vm/#deploying-centos","text":"In Prism Central > select bars {.interpreted-text role=\"fa\"} > Virtual Infrastructure > VMs , and click Create VM . Fill out the following fields: Name - Initials -Linux-ToolsVM Description - (Optional) Description for your VM. vCPU(s) - 1 Number of Cores per vCPU - 2 Memory - 2 GiB Select + Add New Disk : - Type - DISK - Operation - Clone from Image Service - Image - CentOS7.qcow2 - Select Add Select Add New NIC : - VLAN Name - Secondary - Select Add Click Save to create the VM. Power On the VM.","title":"Deploying CentOS"},{"location":"tools_vms/linux_tools_vm/#installing-tools","text":"Login to the VM via ssh or Console session, using the following credentials: Username - root password - nutanix/4u Install the software needed by running the following commands: yum update -y yum install -y ntp ntpdate unzip stress nodejs python-pip s3cmd awscli npm install -g request npm install -g express","title":"Installing Tools"},{"location":"tools_vms/linux_tools_vm/#configuring-ntp","text":"Enable and configure NTP by running the following commands: systemctl start ntpd systemctl enable ntpd ntpdate -u -s 0 .pool.ntp.org 1 .pool.ntp.org 2 .pool.ntp.org 3 .pool.ntp.org systemctl restart ntpd","title":"Configuring NTP"},{"location":"tools_vms/linux_tools_vm/#disabling-firewall-and-selinux","text":"Disable the firewall and SELinux by running the following commands: systemctl disable firewalld systemctl stop firewalld setenforce 0 sed -i 's/enforcing/disabled/g' /etc/selinux/config /etc/selinux/config","title":"Disabling Firewall and SELinux"},{"location":"tools_vms/linux_tools_vm/#installing-python","text":"If completing the apis {.interpreted-text role=\"ref\"} lab using the Linux Tools VM, install Python by running the following commands: yum -y install python36 python3.6 -m ensurepip yum -y install python36-setuptools pip install -U pip pip install boto3","title":"Installing Python"},{"location":"tools_vms/windows_tools_vm/","text":"Overview Deploy this Windows 10 VM on your assigned cluster if directed to do so as part of Lab Setup . <strong><font color=\"red\">Only deploy the VM once, it does not need to be cleaned up as part of any lab completion.</font></strong> Deploying Tools VM Using an SSH client, connect to the Node A CVM IP \\<10.42.xx.29> in your assigned block using the following credentials: Username - nutanix Password - default Execute the following commands to upload AD image: acli image.create Windows10 container = Images image_type = kDiskImage source_url = https://s3.amazonaws.com/get-ahv-images/Windows10-1709.qcow2 In Prism Central > select bars {.interpreted-text role=\"fa\"} > Virtual Infrastructure > VMs , and click Create VM . Fill out the following fields: Name - Initials -Windows-ToolsVM Description - (Optional) Description for your VM. vCPU(s) - 1 Number of Cores per vCPU - 2 Memory - 4 GiB Select + Add New Disk : - Type - DISK - Operation - Clone from Image Service - Image - Windows10-1709.qcow2 - Select Add Select Add New NIC : - VLAN Name - Secondary - Select Add Click Save to create the VM. Power On the VM. Login to the VM via RDP or Console session, using the following credentials: Username - NTNXLAB\\Administrator password - nutanix/4u","title":"Windows Tools VM"},{"location":"tools_vms/windows_tools_vm/#overview","text":"Deploy this Windows 10 VM on your assigned cluster if directed to do so as part of Lab Setup . <strong><font color=\"red\">Only deploy the VM once, it does not need to be cleaned up as part of any lab completion.</font></strong>","title":"Overview"},{"location":"tools_vms/windows_tools_vm/#deploying-tools-vm","text":"Using an SSH client, connect to the Node A CVM IP \\<10.42.xx.29> in your assigned block using the following credentials: Username - nutanix Password - default Execute the following commands to upload AD image: acli image.create Windows10 container = Images image_type = kDiskImage source_url = https://s3.amazonaws.com/get-ahv-images/Windows10-1709.qcow2 In Prism Central > select bars {.interpreted-text role=\"fa\"} > Virtual Infrastructure > VMs , and click Create VM . Fill out the following fields: Name - Initials -Windows-ToolsVM Description - (Optional) Description for your VM. vCPU(s) - 1 Number of Cores per vCPU - 2 Memory - 4 GiB Select + Add New Disk : - Type - DISK - Operation - Clone from Image Service - Image - Windows10-1709.qcow2 - Select Add Select Add New NIC : - VLAN Name - Secondary - Select Add Click Save to create the VM. Power On the VM. Login to the VM via RDP or Console session, using the following credentials: Username - NTNXLAB\\Administrator password - nutanix/4u","title":"Deploying Tools VM"},{"location":"xray/xray/","text":"Overview Note Estimated time to complete: 60 Minutes X-Ray is an automated testing application for virtualized infrastructure solutions. It is capable of running test scenarios end-to-end to evaluate system attributes in real-world use cases. In this exercise you will deploy and configure an X-Ray VM, run X-Ray tests, and analyze results. As X-Ray powers down hosts for tests that evaluate availability and data ntegrity, it is best practice to run the X-Ray VM outside of the target cluster. Additionally, the X-Ray VM itself creates a small amount of storage and CPU overhead that could potentially skew results. In this lab, we will deploy X-Ray VM on POCxx-D, and evalutate cluster POCxx-ABC we just created. For environments where DHCP is unavailable (or there isn't a sufficiently large pool of addresses available), X-Ray supports Link-local or Zero Configuration networking, where the VMs communicate via self-assigned IPv4 addresses. In order to work, all of the VMs (including the X-Ray VM) need to reside on the same Layer 2 network. To use Link-local networking, your X-Ray VM's first NIC (eth0) should be on a network capable of communicating with your cluster. A second NIC (eth1) is added on a network without DHCP. Create the X-Ray VM image Open a terminal and SSH to Node-D CVM, enter CVM credentials and execute following commands Logon to SSH console of CVM ssh nutanix@10.42.xx.32 #<check password in RX> Upload the X-Ray Image acli image.create X-Ray container = Images image_type = kDiskImage source_url = http://10.42.194.11/images/Xray/4.1.3/xray-4.1.3.qcow2 Caution Wait until you see that the image upload is complete with a message X-Ray: Complete You can confirm presence of X-Ray image by running the following command in the same shell nutanix@10.42.79.32:~$ acli image.list # Output here Image name Image type Image UUID Foundation kDiskImage c970941a-d583-4640-8e03-9b2ca7336d00 X-Ray kDiskImage ac819fab-3fb9-4e85-99fd-97ca3f925ec8 << here is your X-Ray Image Configuring Networks For targeting network, we will use the Secondary network VLAN for communication between the X-Ray VM and X-Ray worker VMs. This is accomplished via \"Zero Configuration\" networking, as the 3-node cluster Secondary and 1-node cluster Secondary networks are the same Layer 2 network and there is no DHCP. Now we switch to Prism portal of single node cluster D Open https://<POCxx-D Cluster IP>:9440 (https://10.42.xx.32:9440) in your browser and log in with the following credentials: Username - admin Password - check password in RX Then, configue the Secondary network on the single node cluster D Click Create Network . Using the Cluster Details spreadsheet, fill out the following fields and click Save : Name - Secondary Virtual Switch - vs0 VLAN ID - HPOC Cluster ID 1 (e.g. for PHX-POC079 , VLAN ID would be 791 ) Enable IP address management - leave it unselected Click on Save Creating X-Ray VM In Prism > VM > Table and click + Create VM . Fill out the following fields and click Save : Name - X-Ray vCPU(s) - 2 Number of Cores per vCPU - 1 Memory - 4 GiB Select + Add New Disk Operation - Clone from Image Service Image - X-Ray Select Add Select Add New NIC VLAN Name - Primary Select Add Select + Add New NIC VLAN Name - Secondary Select Add Select your X-Ray VM and click Power on . Info At the time of writing, X-Ray 4.1.3 is the latest available version. The URL for the latest X-Ray OVA & QCOW2 images can be downloaded from the Nutanix Portal . Once the VM has started, click Launch Console Make sure the VM is booting to console Your X-Ray VM would have received an IP address from the DHCP server in Primary network Determine the IP address of the NIC (eth0) on the Primary network of the X-Ray VM from Prism Element and note it down Note It is critical that you select the IP address of the network adapter assigned to the Primary network (you can confirm by comparing the MAC address in the VM console to the MAC address shown in Prism). We will use this network to assign a static IP to the X-Ray VM to access the web interface. We will NOT assign an address to the Secondary network adapter. This network will be used for zero configuration communication between the X-Ray VM and client VMs. This approach is helpful when DHCP isn't available or the DHCP scope isn't large enough to support X-Ray testing. Configuring X-Ray Open https://X-RAY-VM-IP ( https://10.42.xx.52 ) in a browser Click on Log in with Local Account Click on Sign up now in the bottom of the screen Provide the following details Email - youremail@nutanix.com Password - set it to cluster password Click on Submit Select I have read and agree to the terms and conditions and click Accept . Select Targets from the navigation bar and click Add Target . Fill out the following fields and click Next : Name - POCxx-ABC Manager Type - Prism Cluster Type - Nutanix Hypervisor - AHV Address - 3-Node Cluster Virtual IP 10.42.xx.37 Username - admin Password - you 3 node cluster password Expiration Time - leave blank Click Next Select Secondary under Network and click Next . Click Next Under OOB Management Protocol , choose IPMI Review A, B and C node configurations From the drop down menu, choose Fill with Nutanix defaults Click Next Click Run Validation . Click Check Details to view validation progress. Upon successful completion of validation, click Done . Running X-Ray Tests While X-Ray offers many testing options, we will use Peak Performance Microbenchmark test in this lab. Select Tests from the navigation bar In the list of tests, find Peak Performance Microbenchmark and click on View & Run Test Review the test description Enter the test name as \\<Your Initials> - Peak Perforamce Test Confirm your POCxx-ABC as the right target Choose the Default test variant Click Run test . Caution X-Ray can run one test per target at a time. Many tests can be queued for a single target, allowing X-Ray to automatically run through multiple tests without requiring manual intervention. Through automation, X-Ray can drastically decrease the amount of time to conduct a POC. Click on View Test You are able to monitor the test progress and results in the Results page Click on In Progress link to see which stage you are at in the test You can see the random/sequential read/write tests are coming up soon Click on Got it to return to the test Results page As the test runs you will be able to see the test results as shown here Note the minimum and maximum performance numbers all in one screen. You are also able to get detailed view of the metrics as a Grafana dashboard, click on the Grafana Dashboard link Grafana dashboards presents detailed metrics view You are also able to generate reports (in PDF), export Test Results and re-run the tests. Tip The graphs are interactive, and you can click and drag to zoom into specific data/times on each individual graph. You can zoom out by clicking Reset Zoom . Each dotted blue line represents an event in the test, such as beginning a workload, powering off a node, etc. Clicking the blue dots will provide information about the event. Clicking the Actions drop down menu provides options to view the detailed log data, export the test results, and generate a PDF report. Working with X-Ray Results As X-Ray is using automation to perform the exact same tests and collect the same metrics on multiple systems/hypervisors, the results can be easily overlaid to compare solutions. In this exercise you will use X-Ray to compare BigData Ingestion test results between Nutanix and a competitor. The BigData Ingestion test compares the speed at which 1TB of sequential data can be written to a single VM on a cluster, as is common in workloads such as Splunk. Download the following exported X-Ray test result: Competitor + Nutanix Big Data Ingest Results Select Results > Import Test Result Bundle from the navigation bar. Click Choose File and select the Nutanix test results .zip file previously downloaded. Click Upload . Once the file successfully uploads, you will see three results as shown here Select all 3 BigData Ingestion results and click Create Comparison . The resulting charts show the combined metrics for all three solutions. You are able to see the software and hardware versions of the infrastructure cluster where the test was conducted. By hovering over the graph you can also see point-in-time performance metrics. In this case we can clearly see that the Nutanix solution is able to sustain a higher, and more consistent, rate of write throughput, resulting in a much faster time to complete ingesting the 1TB of data. Exporting X-Ray Results To export analysis results for use in proposal documents, etc., Select the results you need in All Results page and click on Create report . The results will open and can be printed or exported as PDF. Here is a results file for your reference. X-Ray Export Result PDF Multiple analyses can also be selected to generate a combined report with the results from multiple tests, this can be extremely useful for summarizing POC results. Question Can you explain why the Nutanix solution may produce better results than common HCI competitors? Tip Check out the OpLog section of the Nutanix Bible Takeaways X-Ray is a easy to use benchmarking tool to make your life in the field easier X-Ray has many testing scenarios (database, big data, etc) for specific use-cases X-Ray is available as a VM appliance as well as SaaS (tests can be run only on Nutanix HPOC clusters) X-Ray testing parameters can be customised easily to suit yours or your customer's testing requierements","title":"X-Ray"},{"location":"xray/xray/#overview","text":"Note Estimated time to complete: 60 Minutes X-Ray is an automated testing application for virtualized infrastructure solutions. It is capable of running test scenarios end-to-end to evaluate system attributes in real-world use cases. In this exercise you will deploy and configure an X-Ray VM, run X-Ray tests, and analyze results. As X-Ray powers down hosts for tests that evaluate availability and data ntegrity, it is best practice to run the X-Ray VM outside of the target cluster. Additionally, the X-Ray VM itself creates a small amount of storage and CPU overhead that could potentially skew results. In this lab, we will deploy X-Ray VM on POCxx-D, and evalutate cluster POCxx-ABC we just created. For environments where DHCP is unavailable (or there isn't a sufficiently large pool of addresses available), X-Ray supports Link-local or Zero Configuration networking, where the VMs communicate via self-assigned IPv4 addresses. In order to work, all of the VMs (including the X-Ray VM) need to reside on the same Layer 2 network. To use Link-local networking, your X-Ray VM's first NIC (eth0) should be on a network capable of communicating with your cluster. A second NIC (eth1) is added on a network without DHCP.","title":"Overview"},{"location":"xray/xray/#create-the-x-ray-vm-image","text":"Open a terminal and SSH to Node-D CVM, enter CVM credentials and execute following commands Logon to SSH console of CVM ssh nutanix@10.42.xx.32 #<check password in RX> Upload the X-Ray Image acli image.create X-Ray container = Images image_type = kDiskImage source_url = http://10.42.194.11/images/Xray/4.1.3/xray-4.1.3.qcow2 Caution Wait until you see that the image upload is complete with a message X-Ray: Complete You can confirm presence of X-Ray image by running the following command in the same shell nutanix@10.42.79.32:~$ acli image.list # Output here Image name Image type Image UUID Foundation kDiskImage c970941a-d583-4640-8e03-9b2ca7336d00 X-Ray kDiskImage ac819fab-3fb9-4e85-99fd-97ca3f925ec8 << here is your X-Ray Image","title":"Create the X-Ray VM image"},{"location":"xray/xray/#configuring-networks","text":"For targeting network, we will use the Secondary network VLAN for communication between the X-Ray VM and X-Ray worker VMs. This is accomplished via \"Zero Configuration\" networking, as the 3-node cluster Secondary and 1-node cluster Secondary networks are the same Layer 2 network and there is no DHCP. Now we switch to Prism portal of single node cluster D Open https://<POCxx-D Cluster IP>:9440 (https://10.42.xx.32:9440) in your browser and log in with the following credentials: Username - admin Password - check password in RX Then, configue the Secondary network on the single node cluster D Click Create Network . Using the Cluster Details spreadsheet, fill out the following fields and click Save : Name - Secondary Virtual Switch - vs0 VLAN ID - HPOC Cluster ID 1 (e.g. for PHX-POC079 , VLAN ID would be 791 ) Enable IP address management - leave it unselected Click on Save","title":"Configuring Networks"},{"location":"xray/xray/#creating-x-ray-vm","text":"In Prism > VM > Table and click + Create VM . Fill out the following fields and click Save : Name - X-Ray vCPU(s) - 2 Number of Cores per vCPU - 1 Memory - 4 GiB Select + Add New Disk Operation - Clone from Image Service Image - X-Ray Select Add Select Add New NIC VLAN Name - Primary Select Add Select + Add New NIC VLAN Name - Secondary Select Add Select your X-Ray VM and click Power on . Info At the time of writing, X-Ray 4.1.3 is the latest available version. The URL for the latest X-Ray OVA & QCOW2 images can be downloaded from the Nutanix Portal . Once the VM has started, click Launch Console Make sure the VM is booting to console Your X-Ray VM would have received an IP address from the DHCP server in Primary network Determine the IP address of the NIC (eth0) on the Primary network of the X-Ray VM from Prism Element and note it down Note It is critical that you select the IP address of the network adapter assigned to the Primary network (you can confirm by comparing the MAC address in the VM console to the MAC address shown in Prism). We will use this network to assign a static IP to the X-Ray VM to access the web interface. We will NOT assign an address to the Secondary network adapter. This network will be used for zero configuration communication between the X-Ray VM and client VMs. This approach is helpful when DHCP isn't available or the DHCP scope isn't large enough to support X-Ray testing.","title":"Creating X-Ray VM"},{"location":"xray/xray/#configuring-x-ray","text":"Open https://X-RAY-VM-IP ( https://10.42.xx.52 ) in a browser Click on Log in with Local Account Click on Sign up now in the bottom of the screen Provide the following details Email - youremail@nutanix.com Password - set it to cluster password Click on Submit Select I have read and agree to the terms and conditions and click Accept . Select Targets from the navigation bar and click Add Target . Fill out the following fields and click Next : Name - POCxx-ABC Manager Type - Prism Cluster Type - Nutanix Hypervisor - AHV Address - 3-Node Cluster Virtual IP 10.42.xx.37 Username - admin Password - you 3 node cluster password Expiration Time - leave blank Click Next Select Secondary under Network and click Next . Click Next Under OOB Management Protocol , choose IPMI Review A, B and C node configurations From the drop down menu, choose Fill with Nutanix defaults Click Next Click Run Validation . Click Check Details to view validation progress. Upon successful completion of validation, click Done .","title":"Configuring X-Ray"},{"location":"xray/xray/#running-x-ray-tests","text":"While X-Ray offers many testing options, we will use Peak Performance Microbenchmark test in this lab. Select Tests from the navigation bar In the list of tests, find Peak Performance Microbenchmark and click on View & Run Test Review the test description Enter the test name as \\<Your Initials> - Peak Perforamce Test Confirm your POCxx-ABC as the right target Choose the Default test variant Click Run test . Caution X-Ray can run one test per target at a time. Many tests can be queued for a single target, allowing X-Ray to automatically run through multiple tests without requiring manual intervention. Through automation, X-Ray can drastically decrease the amount of time to conduct a POC. Click on View Test You are able to monitor the test progress and results in the Results page Click on In Progress link to see which stage you are at in the test You can see the random/sequential read/write tests are coming up soon Click on Got it to return to the test Results page As the test runs you will be able to see the test results as shown here Note the minimum and maximum performance numbers all in one screen. You are also able to get detailed view of the metrics as a Grafana dashboard, click on the Grafana Dashboard link Grafana dashboards presents detailed metrics view You are also able to generate reports (in PDF), export Test Results and re-run the tests. Tip The graphs are interactive, and you can click and drag to zoom into specific data/times on each individual graph. You can zoom out by clicking Reset Zoom . Each dotted blue line represents an event in the test, such as beginning a workload, powering off a node, etc. Clicking the blue dots will provide information about the event. Clicking the Actions drop down menu provides options to view the detailed log data, export the test results, and generate a PDF report.","title":"Running X-Ray Tests"},{"location":"xray/xray/#working-with-x-ray-results","text":"As X-Ray is using automation to perform the exact same tests and collect the same metrics on multiple systems/hypervisors, the results can be easily overlaid to compare solutions. In this exercise you will use X-Ray to compare BigData Ingestion test results between Nutanix and a competitor. The BigData Ingestion test compares the speed at which 1TB of sequential data can be written to a single VM on a cluster, as is common in workloads such as Splunk. Download the following exported X-Ray test result: Competitor + Nutanix Big Data Ingest Results Select Results > Import Test Result Bundle from the navigation bar. Click Choose File and select the Nutanix test results .zip file previously downloaded. Click Upload . Once the file successfully uploads, you will see three results as shown here Select all 3 BigData Ingestion results and click Create Comparison . The resulting charts show the combined metrics for all three solutions. You are able to see the software and hardware versions of the infrastructure cluster where the test was conducted. By hovering over the graph you can also see point-in-time performance metrics. In this case we can clearly see that the Nutanix solution is able to sustain a higher, and more consistent, rate of write throughput, resulting in a much faster time to complete ingesting the 1TB of data.","title":"Working with X-Ray Results"},{"location":"xray/xray/#exporting-x-ray-results","text":"To export analysis results for use in proposal documents, etc., Select the results you need in All Results page and click on Create report . The results will open and can be printed or exported as PDF. Here is a results file for your reference. X-Ray Export Result PDF Multiple analyses can also be selected to generate a combined report with the results from multiple tests, this can be extremely useful for summarizing POC results. Question Can you explain why the Nutanix solution may produce better results than common HCI competitors? Tip Check out the OpLog section of the Nutanix Bible","title":"Exporting X-Ray Results"},{"location":"xray/xray/#takeaways","text":"X-Ray is a easy to use benchmarking tool to make your life in the field easier X-Ray has many testing scenarios (database, big data, etc) for specific use-cases X-Ray is available as a VM appliance as well as SaaS (tests can be run only on Nutanix HPOC clusters) X-Ray testing parameters can be customised easily to suit yours or your customer's testing requierements","title":"Takeaways"}]}